{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import mean_iou_evaluate\n",
    "import viz_mask\n",
    "import imageio\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HW2IMGS(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        \" initial the dataset \"\n",
    "        self.X_image = None\n",
    "        self.y_label = None\n",
    "        self.X_filenames = []\n",
    "        self.y_filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "\n",
    "        # read filenames\n",
    "        X_filenames = glob.glob(root+'/*.jpg')\n",
    "        y_filenames = glob.glob(root+'/*.png')\n",
    "        for i in range(len(X_filenames)):\n",
    "            self.X_filenames.append(os.path.splitext(\n",
    "                os.path.basename(X_filenames[i]))[0])\n",
    "            self.y_filenames.append(os.path.splitext(\n",
    "                os.path.basename(y_filenames[i]))[0])\n",
    "\n",
    "        self.len = len(self.X_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_filename, y_filename = self.X_filenames[index], self.y_filenames[index]\n",
    "\n",
    "        X_image = Image.open(self.root+X_filename+'.jpg')\n",
    "        X_shape = imageio.imread(self.root+X_filename+'.jpg').shape\n",
    "\n",
    "        y_image = imageio.imread(self.root+y_filename+'.png')\n",
    "        y_label = viz_mask.read_masks(y_image, X_shape)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X_image = self.transform(X_image)\n",
    "\n",
    "        \" get a sample from the dataset \"\n",
    "        # if torch.cuda.is_available():\n",
    "        #   X_image, y_label = X_image.cuda(), y_label.cuda()\n",
    "\n",
    "        return X_image, y_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \" Total number of sampler in the dataset \"\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "trainset = HW2IMGS(root='p2_data/train/', transform=transforms.Compose([\n",
    "    # transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "]))\n",
    "\n",
    "valset = HW2IMGS(root='p2_data/validation/', transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "]))\n",
    "print(len(trainset))\n",
    "print(len(valset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_loader = DataLoader(trainset, batch_size=1, shuffle=True)\n",
    "valset_loader = DataLoader(valset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN32(\n",
      "  (vgg_feature): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (vgg_fc): Sequential(\n",
      "    (0): Conv2d(512, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(4096, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(7, 7, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "# summary(vgg, (3, 512, 512))\n",
    "\n",
    "\n",
    "class FCN32(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN32, self).__init__()\n",
    "        self.vgg_feature = models.vgg16(pretrained=True).features\n",
    "\n",
    "        self.vgg_fc = nn.Sequential(\n",
    "            nn.Conv2d(512, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4096, 4096, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4096, 7, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(7, 7, 32, 32),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg_feature(x)\n",
    "        x = self.vgg_fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = FCN32().to(device)  # Remember to move the model to \"device\"\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "              ReLU-2         [-1, 64, 512, 512]               0\n",
      "            Conv2d-3         [-1, 64, 512, 512]          36,928\n",
      "              ReLU-4         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-5         [-1, 64, 256, 256]               0\n",
      "            Conv2d-6        [-1, 128, 256, 256]          73,856\n",
      "              ReLU-7        [-1, 128, 256, 256]               0\n",
      "            Conv2d-8        [-1, 128, 256, 256]         147,584\n",
      "              ReLU-9        [-1, 128, 256, 256]               0\n",
      "        MaxPool2d-10        [-1, 128, 128, 128]               0\n",
      "           Conv2d-11        [-1, 256, 128, 128]         295,168\n",
      "             ReLU-12        [-1, 256, 128, 128]               0\n",
      "           Conv2d-13        [-1, 256, 128, 128]         590,080\n",
      "             ReLU-14        [-1, 256, 128, 128]               0\n",
      "           Conv2d-15        [-1, 256, 128, 128]         590,080\n",
      "             ReLU-16        [-1, 256, 128, 128]               0\n",
      "        MaxPool2d-17          [-1, 256, 64, 64]               0\n",
      "           Conv2d-18          [-1, 512, 64, 64]       1,180,160\n",
      "             ReLU-19          [-1, 512, 64, 64]               0\n",
      "           Conv2d-20          [-1, 512, 64, 64]       2,359,808\n",
      "             ReLU-21          [-1, 512, 64, 64]               0\n",
      "           Conv2d-22          [-1, 512, 64, 64]       2,359,808\n",
      "             ReLU-23          [-1, 512, 64, 64]               0\n",
      "        MaxPool2d-24          [-1, 512, 32, 32]               0\n",
      "           Conv2d-25          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-26          [-1, 512, 32, 32]               0\n",
      "           Conv2d-27          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-28          [-1, 512, 32, 32]               0\n",
      "           Conv2d-29          [-1, 512, 32, 32]       2,359,808\n",
      "             ReLU-30          [-1, 512, 32, 32]               0\n",
      "        MaxPool2d-31          [-1, 512, 16, 16]               0\n",
      "           Conv2d-32         [-1, 4096, 16, 16]       2,101,248\n",
      "             ReLU-33         [-1, 4096, 16, 16]               0\n",
      "           Conv2d-34         [-1, 4096, 16, 16]      16,781,312\n",
      "             ReLU-35         [-1, 4096, 16, 16]               0\n",
      "           Conv2d-36            [-1, 7, 16, 16]          28,679\n",
      "             ReLU-37            [-1, 7, 16, 16]               0\n",
      "  ConvTranspose2d-38          [-1, 7, 512, 512]          50,183\n",
      "================================================================\n",
      "Total params: 33,676,110\n",
      "Trainable params: 33,676,110\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 1187.03\n",
      "Params size (MB): 128.46\n",
      "Estimated Total Size (MB): 1318.49\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 512, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_acc_his, train_loss_his = [], []\n",
    "val_acc_his, val_loss_his = [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval=100):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()  # Important: set training mode\n",
    "    for ep in range(epoch):\n",
    "        iteration = 0\n",
    "        correct = 0\n",
    "        total_train = 0\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(trainset_loader)):\n",
    "\n",
    "            loss = 0.0\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            # print(output.shape)\n",
    "\n",
    "            output = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            target = torch.tensor(target, dtype=torch.long, device=device)\n",
    "            loss = criterion(output, target)\n",
    "            # print(mean_iou_evaluate.mean_iou_score(output,target))\n",
    "    #         # print(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # get the index of the max log-probability\n",
    "\n",
    "            total_train += target.nelement()\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            pred = pred.eq(target.view_as(pred))\n",
    "            # print(pred)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            iteration+=1\n",
    "\n",
    "            if (iteration % log_interval == 0):\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "                    ep+1, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item(),\n",
    "                    correct, total_train,\n",
    "                    100. * correct / total_train))\n",
    "                \n",
    "\n",
    "        train_acc_his.append(100. * correct / len(trainset_loader.dataset))\n",
    "        train_loss_his.append(loss.item())\n",
    "        val(model)  # Evaluate at the end of each epoch\n",
    "        scheduler.step()\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(train_acc_his)\n",
    "    plt.plot(val_acc_his)\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss')\n",
    "    plt.plot(train_loss_his)\n",
    "    plt.plot(val_loss_his)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()  # Important: set evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():  # This will free the GPU memory used for back-prop\n",
    "        for data, target in valset_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            output = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            # print(output.shape, target.shape)\n",
    "            target = torch.tensor(target, dtype=torch.long, device=device)\n",
    "            # target.clone().detach()\n",
    "            val_loss += criterion(output, target)\n",
    "            total_val += target.nelement()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss /= len(valset_loader.dataset)\n",
    "    print('\\nVal set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, total_val,\n",
    "        100. * correct / total_val))\n",
    "\n",
    "    val_acc_his.append(100. * correct / len(valset_loader.dataset))\n",
    "    val_loss_his.append(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]<ipython-input-9-b009ef562ba0>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target, dtype=torch.long, device=device)\n",
      "  5%|▌         | 100/2000 [00:29<09:24,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [99/2000 (5%)]\tLoss: 1.737273, Accuracy: 7197234/26214400 (27%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/2000 [00:59<09:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [199/2000 (10%)]\tLoss: 1.553814, Accuracy: 17036594/52428800 (32%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [01:30<08:45,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [299/2000 (15%)]\tLoss: 0.680486, Accuracy: 30957814/78643200 (39%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:01<08:20,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [399/2000 (20%)]\tLoss: 2.242883, Accuracy: 46220905/104857600 (44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [02:33<07:54,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [499/2000 (25%)]\tLoss: 0.371696, Accuracy: 63833263/131072000 (49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [03:04<07:24,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [599/2000 (30%)]\tLoss: 1.612338, Accuracy: 82483746/157286400 (52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 700/2000 [03:36<06:52,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [699/2000 (35%)]\tLoss: 1.545923, Accuracy: 100514655/183500800 (55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [04:08<06:20,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [799/2000 (40%)]\tLoss: 2.238561, Accuracy: 119329616/209715200 (57%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 900/2000 [04:39<05:47,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [899/2000 (45%)]\tLoss: 1.170566, Accuracy: 137916684/235929600 (58%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [05:11<05:16,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [999/2000 (50%)]\tLoss: 0.965901, Accuracy: 155442172/262144000 (59%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1100/2000 [05:43<04:45,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1099/2000 (55%)]\tLoss: 0.267663, Accuracy: 172684700/288358400 (60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1200/2000 [06:14<04:13,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1199/2000 (60%)]\tLoss: 1.636092, Accuracy: 190644081/314572800 (61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1300/2000 [06:46<03:41,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1299/2000 (65%)]\tLoss: 0.168801, Accuracy: 208571900/340787200 (61%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1400/2000 [07:18<03:10,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1399/2000 (70%)]\tLoss: 1.281055, Accuracy: 226785676/367001600 (62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1500/2000 [07:49<02:37,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1499/2000 (75%)]\tLoss: 0.026740, Accuracy: 245321242/393216000 (62%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1600/2000 [08:21<02:06,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1599/2000 (80%)]\tLoss: 0.123905, Accuracy: 262302310/419430400 (63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1700/2000 [08:53<01:34,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1699/2000 (85%)]\tLoss: 0.729093, Accuracy: 280168403/445644800 (63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1800/2000 [09:24<01:03,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1799/2000 (90%)]\tLoss: 0.218040, Accuracy: 297341303/471859200 (63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1900/2000 [09:56<00:31,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1899/2000 (95%)]\tLoss: 0.425644, Accuracy: 316716911/498073600 (64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [10:27<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1999/2000 (100%)]\tLoss: 1.959495, Accuracy: 335323689/524288000 (64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-10-a59cc59ac628>:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target = torch.tensor(target, dtype=torch.long, device=device)\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val set: Average loss: 0.9776, Accuracy: 42040401/67371008 (62%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 100/2000 [00:31<09:45,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [99/2000 (5%)]\tLoss: 2.444502, Accuracy: 16844424/26214400 (64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/2000 [01:02<09:14,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [199/2000 (10%)]\tLoss: 0.584470, Accuracy: 34980699/52428800 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 300/2000 [01:33<08:48,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [299/2000 (15%)]\tLoss: 0.393927, Accuracy: 53064918/78643200 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [02:05<09:59,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [399/2000 (20%)]\tLoss: 0.495505, Accuracy: 69388791/104857600 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [02:36<07:44,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [499/2000 (25%)]\tLoss: 1.904156, Accuracy: 87155969/131072000 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [03:07<07:29,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [599/2000 (30%)]\tLoss: 1.207604, Accuracy: 104299301/157286400 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 700/2000 [03:38<06:47,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [699/2000 (35%)]\tLoss: 0.991631, Accuracy: 122392243/183500800 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [04:10<06:17,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [799/2000 (40%)]\tLoss: 0.129975, Accuracy: 139238827/209715200 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 900/2000 [04:41<05:38,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [899/2000 (45%)]\tLoss: 0.672133, Accuracy: 157179474/235929600 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [05:12<05:09,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [999/2000 (50%)]\tLoss: 2.035240, Accuracy: 174765732/262144000 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1100/2000 [05:43<04:41,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1099/2000 (55%)]\tLoss: 0.130698, Accuracy: 192056812/288358400 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1200/2000 [06:15<04:11,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1199/2000 (60%)]\tLoss: 1.403965, Accuracy: 209904211/314572800 (67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1300/2000 [06:46<03:37,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1299/2000 (65%)]\tLoss: 1.289173, Accuracy: 226291626/340787200 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1400/2000 [07:17<03:07,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1399/2000 (70%)]\tLoss: 1.468959, Accuracy: 241825682/367001600 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1500/2000 [07:48<02:38,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1499/2000 (75%)]\tLoss: 1.366368, Accuracy: 258856888/393216000 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1600/2000 [08:19<02:04,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1599/2000 (80%)]\tLoss: 0.315803, Accuracy: 274793867/419430400 (66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1700/2000 [08:50<01:34,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [1699/2000 (85%)]\tLoss: 1.611269, Accuracy: 291484570/445644800 (65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1799/2000 [09:21<01:01,  3.26it/s]"
     ]
    }
   ],
   "source": [
    "train(model, epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.3770e-01, -4.3821e-01,  2.1104e-01],\n",
      "          [ 6.3527e-01, -4.1212e-01,  4.8293e-01],\n",
      "          [ 1.3069e+00,  1.1946e+00,  1.0628e-01]],\n",
      "\n",
      "         [[ 2.9848e-01, -9.3795e-01, -2.0160e+00],\n",
      "          [-2.3025e-01,  1.0400e+00, -9.9195e-02],\n",
      "          [-9.2293e-01,  7.3687e-02,  1.2225e+00]],\n",
      "\n",
      "         [[ 1.0634e+00, -5.5362e-03, -8.3723e-01],\n",
      "          [-4.3611e-01,  9.8528e-02, -9.3485e-01],\n",
      "          [ 1.0458e-03,  4.4920e-01, -1.5266e+00]],\n",
      "\n",
      "         [[-1.1031e+00,  2.3266e-01,  2.2437e+00],\n",
      "          [-5.2398e-01, -1.2317e+00,  1.5176e+00],\n",
      "          [ 3.6768e-03,  1.6619e+00, -2.6280e-01]],\n",
      "\n",
      "         [[-1.2646e+00, -4.5298e-01,  8.7688e-02],\n",
      "          [-1.1462e-01,  1.1270e-01, -1.3377e+00],\n",
      "          [ 2.1916e+00,  4.2946e-01,  5.7133e-01]],\n",
      "\n",
      "         [[-4.7802e-01, -1.6848e+00,  2.5766e-01],\n",
      "          [ 4.0986e-01, -7.2000e-01,  5.5814e-01],\n",
      "          [-1.5496e+00, -4.0597e-01, -6.4538e-01]],\n",
      "\n",
      "         [[ 8.2795e-02,  8.6383e-01, -6.0936e-01],\n",
      "          [ 3.3994e-01,  2.7365e+00,  9.7156e-02],\n",
      "          [ 3.6600e-01, -9.4494e-01, -9.7809e-01]]]])\n",
      "torch.Size([1, 7, 3, 3])\n",
      "tensor([[[[-1.9051, -2.3079, -2.4393],\n",
      "          [-1.4090, -3.4998, -1.8772],\n",
      "          [-1.5086, -1.4306, -1.9936]],\n",
      "\n",
      "         [[-1.7443, -2.8077, -4.6663],\n",
      "          [-2.2746, -2.0477, -2.4593],\n",
      "          [-3.7384, -2.5515, -0.8774]],\n",
      "\n",
      "         [[-0.9794, -1.8753, -3.4875],\n",
      "          [-2.4804, -2.9891, -3.2950],\n",
      "          [-2.8144, -2.1760, -3.6265]],\n",
      "\n",
      "         [[-3.1459, -1.6371, -0.4066],\n",
      "          [-2.5683, -4.3193, -0.8425],\n",
      "          [-2.8118, -0.9632, -2.3627]],\n",
      "\n",
      "         [[-3.3074, -2.3227, -2.5626],\n",
      "          [-2.1589, -2.9750, -3.6978],\n",
      "          [-0.6238, -2.1957, -1.5285]],\n",
      "\n",
      "         [[-2.5208, -3.5545, -2.3927],\n",
      "          [-1.6344, -3.8077, -1.8020],\n",
      "          [-4.3651, -3.0311, -2.7453]],\n",
      "\n",
      "         [[-1.9600, -1.0059, -3.2597],\n",
      "          [-1.7044, -0.3512, -2.2630],\n",
      "          [-2.4495, -3.5701, -3.0780]]]])\n"
     ]
    }
   ],
   "source": [
    "# test code for Mean iou\n",
    "a = torch.randn(1, 7, 3, 3)\n",
    "# print(a)\n",
    "print(a)\n",
    "a = torch.torch.nn.functional.log_softmax(a, dim=1)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "a = a.numpy()\n",
    "# print(a.shape)\n",
    "# b = torch.randn(7,512,512)\n",
    "# b = torch.argmax(b,dim = 0)\n",
    "# b = b.numpy()\n",
    "# mean_iou_evaluate.mean_iou_score(a,b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/32506912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/ching-i/fully-convolutional-networks-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-246aa68ce4ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class VGG16FCN32(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.feats = models.vgg16(pretrained=True).features\n",
    "#         self.fconn = nn.Sequential(\n",
    "#             nn.Conv2d(512, 4096, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             nn.Conv2d(4096, 4096, 1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(),\n",
    "#             )\n",
    "#         self.score = nn.Conv2d(4096, num_classes, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         feats = self.feats(x)\n",
    "#         fconn = self.fconn(feats)\n",
    "#         score = self.score(fconn)\n",
    "\n",
    "#         return F.upsample_bilinear(score, x.size()[2:])\n",
    "\n",
    "\n",
    "#     class UNetEnc(nn.Module):\n",
    "\n",
    "#         def __init__(self, in_channels, features, out_channels):\n",
    "#             super().__init__()\n",
    "\n",
    "#             self.up = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, features, 3),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.Conv2d(features, features, 3),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 nn.ConvTranspose2d(features, out_channels, 2, stride=2),\n",
    "#                 nn.ReLU(inplace=True),\n",
    "#                 )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.up(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/JustinHeaton/fully-convolutional-networks/blob/master/FCN32.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# viz_mask.py\n",
    "'''\n",
    "\n",
    "img = imageio.imread('.\\p2_data\\\\train\\\\0000_mask.png')\n",
    "seg = imageio.imread('.\\p2_data\\\\train\\\\0000_sat.jpg')\n",
    "masks=viz_mask.read_masks(seg, img.shape)\n",
    "\n",
    "print(masks)\n",
    "cs = np.unique(masks)\n",
    "print(cs)\n",
    "cmap = viz_mask.cls_color\n",
    "for c in cs:\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]))\n",
    "    ind = np.where(masks==c)\n",
    "    mask[ind[0], ind[1]] = 1\n",
    "    img = viz_mask.viz_data(img, mask, color=cmap[c])\n",
    "    # print(img[0].shape)\n",
    "    # imageio.imsave('./exp.png', np.uint8(img))\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.malaoshi.top/show_1EF53OZZLiWQ.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(7, 512, 512)\n",
    "b = torch.argmax(a, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
